{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac43534-a1b2-49e8-a4ba-f2b7e618ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Automated Data Pipeline for Retail Orders Analytics\n",
    "Implements Bronze-Silver-Gold (Medallion) Architecture\n",
    "\n",
    "Author: Sarah Bani Issa\n",
    "Date: September 2025\n",
    "Purpose: Deloitte Case Study \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f231c2e-3986-488d-baaf-6a975bbaab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration settings for the data pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zip_file_path = \"Case_Study_202309_Data.zip\"\n",
    "        self.database_path = \"retail_dwh.db\"\n",
    "        self.log_level = logging.INFO\n",
    "        \n",
    "        # Schema names for medallion architecture\n",
    "        self.bronze_schema = \"bronze_layer\"\n",
    "        self.silver_schema = \"silver_layer\" \n",
    "        self.gold_schema = \"gold_layer\"\n",
    "        \n",
    "        # Data quality thresholds\n",
    "        self.max_null_percentage = 10  # Max % of nulls allowed\n",
    "        self.min_records_threshold = 100  # Min records per file\n",
    "        \n",
    "        # File naming pattern: YYYYMM_Orders_YYYY_MM_DD_HH_MM_SS\n",
    "        self.file_pattern = r'^(\\d{6})_Orders_(\\d{4})_(\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})\\.csv$'\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('pipeline_execution.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f40856-805b-4815-90e8-5f4735c0de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE CONNECTION MANAGER\n",
    "# ============================================================================\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Manages database connections and operations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get database connection\"\"\"\n",
    "        return sqlite3.connect(self.db_path)\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Execute SQL query\"\"\"\n",
    "        try:\n",
    "            with self.get_connection() as conn:\n",
    "                if params:\n",
    "                    return conn.execute(query, params)\n",
    "                else:\n",
    "                    return conn.execute(query)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Database query failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_schemas(self):\n",
    "        \"\"\"Create database schemas/tables for medallion architecture\"\"\"\n",
    "        self.logger.info(\"Creating database schemas...\")\n",
    "        \n",
    "        # Bronze Layer - Raw data\n",
    "        bronze_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bronze_orders_raw (\n",
    "            row_id INTEGER,\n",
    "            order_id TEXT,\n",
    "            order_date TEXT,\n",
    "            ship_date TEXT,\n",
    "            ship_mode TEXT,\n",
    "            customer_id TEXT,\n",
    "            customer_name TEXT,\n",
    "            segment TEXT,\n",
    "            country TEXT,\n",
    "            city TEXT,\n",
    "            state TEXT,\n",
    "            postal_code TEXT,\n",
    "            region TEXT,\n",
    "            product_id TEXT,\n",
    "            category TEXT,\n",
    "            sub_category TEXT,\n",
    "            product_name TEXT,\n",
    "            sales TEXT,\n",
    "            quantity TEXT,\n",
    "            discount TEXT,\n",
    "            profit TEXT,\n",
    "            source_file TEXT,\n",
    "            load_timestamp TEXT,\n",
    "            file_month TEXT,\n",
    "            file_timestamp TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Silver Layer - Cleaned data\n",
    "        silver_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS silver_orders_clean (\n",
    "            row_id INTEGER,\n",
    "            order_id TEXT NOT NULL,\n",
    "            order_date DATE,\n",
    "            ship_date DATE,\n",
    "            ship_mode TEXT,\n",
    "            customer_id TEXT,\n",
    "            customer_name TEXT,\n",
    "            segment TEXT,\n",
    "            country TEXT,\n",
    "            city TEXT,\n",
    "            state TEXT,\n",
    "            postal_code TEXT,\n",
    "            region TEXT,\n",
    "            product_id TEXT,\n",
    "            category TEXT,\n",
    "            sub_category TEXT,\n",
    "            product_name TEXT,\n",
    "            sales REAL,\n",
    "            quantity INTEGER,\n",
    "            discount REAL,\n",
    "            profit REAL,\n",
    "            delivery_days INTEGER,\n",
    "            data_quality_score REAL,\n",
    "            quality_flags TEXT,\n",
    "            source_file TEXT,\n",
    "            processed_timestamp TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gold Layer - Dimensional model\n",
    "        dim_customer_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS gold_dim_customer (\n",
    "            customer_id INTEGER PRIMARY KEY,\n",
    "            customer_id TEXT UNIQUE,\n",
    "            customer_name TEXT,\n",
    "            segment TEXT,\n",
    "            created_date TEXT,\n",
    "            updated_date TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        dim_product_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS gold_dim_product (\n",
    "            product_id INTEGER PRIMARY KEY,\n",
    "            product_id TEXT UNIQUE,\n",
    "            product_name TEXT,\n",
    "            category TEXT,\n",
    "            sub_category TEXT,\n",
    "            created_date TEXT,\n",
    "            updated_date TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        dim_geography_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS gold_dim_geography (\n",
    "            geography_id INTEGER PRIMARY KEY,\n",
    "            country TEXT,\n",
    "            state TEXT,\n",
    "            city TEXT,\n",
    "            postal_code TEXT,\n",
    "            region TEXT,\n",
    "            created_date TEXT,\n",
    "            UNIQUE(country, state, city, postal_code)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        fact_orders_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS gold_fact_orders (\n",
    "            order_id INTEGER PRIMARY KEY,\n",
    "            order_id TEXT,\n",
    "            customer_id INTEGER,\n",
    "            product_id INTEGER,\n",
    "            geography_id INTEGER,\n",
    "            order_date DATE,\n",
    "            ship_date DATE,\n",
    "            ship_mode TEXT,\n",
    "            sales REAL,\n",
    "            quantity INTEGER,\n",
    "            discount REAL,\n",
    "            profit REAL,\n",
    "            delivery_days INTEGER,\n",
    "            row_id INTEGER,\n",
    "            load_date TEXT,\n",
    "            FOREIGN KEY (customer_id) REFERENCES gold_dim_customer (customer_id),\n",
    "            FOREIGN KEY (product_id) REFERENCES gold_dim_product (product_id),\n",
    "            FOREIGN KEY (geography_id) REFERENCES gold_dim_geography (geography_id)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute DDL statements\n",
    "        ddl_statements = [\n",
    "            bronze_ddl, silver_ddl, dim_customer_ddl, \n",
    "            dim_product_ddl, dim_geography_ddl, fact_orders_ddl\n",
    "        ]\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            for ddl in ddl_statements:\n",
    "                conn.execute(ddl)\n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(\"‚úÖ Database schemas created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f3084-f4e2-4d5e-b95c-d7f063eb34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILE PROCESSOR - Handles ZIP file extraction and latest file selection\n",
    "# ============================================================================\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"Handles file extraction and processing from ZIP archive\"\"\"\n",
    "    \n",
    "    def __init__(self, zip_path, config):\n",
    "        self.zip_path = zip_path\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extract_and_get_latest_files(self):\n",
    "        \"\"\"Extract ZIP and get latest file for each month\"\"\"\n",
    "        self.logger.info(f\"Processing ZIP file: {self.zip_path}\")\n",
    "        \n",
    "        if not os.path.exists(self.zip_path):\n",
    "            raise FileNotFoundError(f\"ZIP file not found: {self.zip_path}\")\n",
    "        \n",
    "        file_info = {}\n",
    "        latest_files = {}\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "                csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "                \n",
    "                self.logger.info(f\"Found {len(csv_files)} CSV files in archive\")\n",
    "                \n",
    "                # Parse file information\n",
    "                for csv_file in csv_files:\n",
    "                    match = re.match(self.config.file_pattern, os.path.basename(csv_file))\n",
    "                    \n",
    "                    if match:\n",
    "                        month_key = match.group(1)  # YYYYMM\n",
    "                        year, month, day, hour, minute, second = match.groups()[1:]\n",
    "                        \n",
    "                        file_timestamp = datetime(\n",
    "                            int(year), int(month), int(day), \n",
    "                            int(hour), int(minute), int(second)\n",
    "                        )\n",
    "                        \n",
    "                        file_info[csv_file] = {\n",
    "                            'month_key': month_key,\n",
    "                            'timestamp': file_timestamp,\n",
    "                            'filename': csv_file\n",
    "                        }\n",
    "                        \n",
    "                        # Keep track of latest file for each month\n",
    "                        if month_key not in latest_files or file_timestamp > latest_files[month_key]['timestamp']:\n",
    "                            latest_files[month_key] = file_info[csv_file]\n",
    "                \n",
    "                # Extract latest files only\n",
    "                extracted_data = []\n",
    "                for month_key, file_data in latest_files.items():\n",
    "                    filename = file_data['filename']\n",
    "                    self.logger.info(f\"Processing latest file for {month_key}: {filename}\")\n",
    "                    \n",
    "                    with zip_ref.open(filename) as file:\n",
    "                        df = pd.read_csv(file, encoding='latin-1', dtype=str)\n",
    "                        df['source_file'] = filename\n",
    "                        df['file_month'] = month_key\n",
    "                        df['file_timestamp'] = file_data['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        df['load_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        \n",
    "                        extracted_data.append(df)\n",
    "                        self.logger.info(f\"‚úÖ Loaded {len(df)} records from {filename}\")\n",
    "                \n",
    "                self.logger.info(f\"‚úÖ Processing complete. Latest files for {len(latest_files)} months\")\n",
    "                return extracted_data, latest_files\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing ZIP file: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef1b71-2fcd-490f-aad5-b656dbdaeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA QUALITY CHECKER\n",
    "# ============================================================================\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Performs data quality validation and scoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.quality_issues = []\n",
    "    \n",
    "    def validate_and_score_data(self, df):\n",
    "        \"\"\"Validate data and assign quality scores\"\"\"\n",
    "        self.logger.info(\"Starting data quality validation...\")\n",
    "        \n",
    "        quality_flags = []\n",
    "        quality_score = 100.0  # Start with perfect score\n",
    "        \n",
    "        # Check 1: Missing critical values\n",
    "        critical_columns = ['Order ID', 'Customer ID', 'Product ID', 'Order Date', 'Sales']\n",
    "        for col in critical_columns:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isna().sum() + (df[col] == '').sum()\n",
    "                null_percentage = (null_count / len(df)) * 100\n",
    "                \n",
    "                if null_percentage > 0:\n",
    "                    quality_flags.append(f\"Missing_{col.replace(' ', '_')}\")\n",
    "                    quality_score -= min(null_percentage * 2, 20)  # Deduct up to 20 points\n",
    "        \n",
    "        # Check 2: Date format issues\n",
    "        date_columns = ['Order Date', 'Ship Date']\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                invalid_dates = 0\n",
    "                for value in df[col].dropna():\n",
    "                    if not self._is_valid_date(value):\n",
    "                        invalid_dates += 1\n",
    "                \n",
    "                if invalid_dates > 0:\n",
    "                    quality_flags.append(f\"Invalid_{col.replace(' ', '_')}\")\n",
    "                    quality_score -= min((invalid_dates / len(df)) * 100, 15)\n",
    "        \n",
    "        # Check 3: Numeric validation\n",
    "        numeric_columns = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                invalid_numeric = 0\n",
    "                for value in df[col].dropna():\n",
    "                    if not self._is_numeric(value):\n",
    "                        invalid_numeric += 1\n",
    "                \n",
    "                if invalid_numeric > 0:\n",
    "                    quality_flags.append(f\"Invalid_{col}\")\n",
    "                    quality_score -= min((invalid_numeric / len(df)) * 100, 10)\n",
    "        \n",
    "        # Check 4: Duplicate Order IDs\n",
    "        if 'Order ID' in df.columns:\n",
    "            duplicates = df['Order ID'].duplicated().sum()\n",
    "            if duplicates > 0:\n",
    "                quality_flags.append(\"Duplicate_Order_IDs\")\n",
    "                quality_score -= min((duplicates / len(df)) * 100, 25)\n",
    "        \n",
    "        quality_score = max(quality_score, 0)  # Don't go below 0\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Data quality validation complete. Average score: {quality_score:.1f}\")\n",
    "        return quality_flags, quality_score\n",
    "    \n",
    "    def _is_valid_date(self, date_str):\n",
    "        \"\"\"Check if string can be parsed as date\"\"\"\n",
    "        date_formats = ['%m/%d/%Y', '%d/%m/%Y', '%Y-%m-%d', '%m-%d-%Y', '%d-%m-%Y']\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                datetime.strptime(str(date_str), fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return False\n",
    "    \n",
    "    def _is_numeric(self, value):\n",
    "        \"\"\"Check if value can be converted to number\"\"\"\n",
    "        try:\n",
    "            float(str(value).replace('$', '').replace(',', '').strip())\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101226e-812f-4567-9db7-fe78f7973b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANER - Silver Layer Processing\n",
    "# ============================================================================\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Handles data cleaning and transformation for Silver layer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Clean and standardize data for Silver layer\"\"\"\n",
    "        self.logger.info(\"Starting data cleaning process...\")\n",
    "        \n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        # Clean dates\n",
    "        cleaned_df = self._clean_dates(cleaned_df)\n",
    "        \n",
    "        # Clean numeric fields\n",
    "        cleaned_df = self._clean_numeric_fields(cleaned_df)\n",
    "        \n",
    "        # Clean text fields\n",
    "        cleaned_df = self._clean_text_fields(cleaned_df)\n",
    "        \n",
    "        # Calculate derived fields\n",
    "        cleaned_df = self._calculate_derived_fields(cleaned_df)\n",
    "        \n",
    "        # Add quality scores\n",
    "        quality_checker = DataQualityChecker()\n",
    "        quality_flags, quality_score = quality_checker.validate_and_score_data(cleaned_df)\n",
    "        \n",
    "        cleaned_df['data_quality_score'] = quality_score\n",
    "        cleaned_df['quality_flags'] = '|'.join(quality_flags) if quality_flags else ''\n",
    "        cleaned_df['processed_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Data cleaning complete. {len(cleaned_df)} records processed\")\n",
    "        return cleaned_df\n",
    "    \n",
    "    def _clean_dates(self, df):\n",
    "        \"\"\"Clean and standardize date columns\"\"\"\n",
    "        date_columns = ['Order Date', 'Ship Date']\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col.lower().replace(' ', '_')] = pd.to_datetime(\n",
    "                    df[col], errors='coerce', infer_datetime_format=True\n",
    "                ).dt.date\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _clean_numeric_fields(self, df):\n",
    "        \"\"\"Clean and convert numeric fields\"\"\"\n",
    "        numeric_mappings = {\n",
    "            'Sales': 'sales',\n",
    "            'Quantity': 'quantity', \n",
    "            'Discount': 'discount',\n",
    "            'Profit': 'profit'\n",
    "        }\n",
    "        \n",
    "        for original_col, new_col in numeric_mappings.items():\n",
    "            if original_col in df.columns:\n",
    "                # Clean and convert to numeric\n",
    "                df[new_col] = pd.to_numeric(\n",
    "                    df[original_col].astype(str).str.replace('[$,]', '', regex=True),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _clean_text_fields(self, df):\n",
    "        \"\"\"Clean and standardize text fields\"\"\"\n",
    "        text_columns = ['Customer Name', 'Product Name', 'City', 'State']\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                new_col = col.lower().replace(' ', '_')\n",
    "                df[new_col] = df[col].astype(str).str.strip().str.title()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_derived_fields(self, df):\n",
    "        \"\"\"Calculate derived fields\"\"\"\n",
    "        # Calculate delivery days\n",
    "        if 'order_date' in df.columns and 'ship_date' in df.columns:\n",
    "            df['delivery_days'] = (\n",
    "                pd.to_datetime(df['ship_date']) - pd.to_datetime(df['order_date'])\n",
    "            ).dt.days\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fc4e3-f7eb-486b-ae42-7e3d5ab41b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIMENSIONAL MODEL BUILDER - Gold Layer\n",
    "# ============================================================================\n",
    "\n",
    "class DimensionalModelBuilder:\n",
    "    \"\"\"Builds dimensional model for Gold layer\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager):\n",
    "        self.db_manager = db_manager\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def build_dimensions_and_facts(self, silver_df):\n",
    "        \"\"\"Build dimensional model from silver layer data\"\"\"\n",
    "        self.logger.info(\"Building dimensional model...\")\n",
    "        \n",
    "        # Build dimensions\n",
    "        customer_dim = self._build_customer_dimension(silver_df)\n",
    "        product_dim = self._build_product_dimension(silver_df)\n",
    "        geography_dim = self._build_geography_dimension(silver_df)\n",
    "        \n",
    "        # Build fact table\n",
    "        fact_orders = self._build_fact_orders(silver_df, customer_dim, product_dim, geography_dim)\n",
    "        \n",
    "        # Load to database\n",
    "        self._load_dimensions_to_db(customer_dim, product_dim, geography_dim)\n",
    "        self._load_facts_to_db(fact_orders)\n",
    "        \n",
    "        return {\n",
    "            'dim_customer': customer_dim,\n",
    "            'dim_product': product_dim, \n",
    "            'dim_geography': geography_dim,\n",
    "            'fact_orders': fact_orders\n",
    "        }\n",
    "    \n",
    "    def _build_customer_dimension(self, df):\n",
    "        \"\"\"Build customer dimension\"\"\"\n",
    "        customer_cols = ['customer_id', 'customer_name', 'segment']\n",
    "        available_cols = [col for col in customer_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            customer_dim = df[available_cols].drop_duplicates().reset_index(drop=True)\n",
    "            customer_dim['customer_id'] = range(1, len(customer_dim) + 1)\n",
    "            customer_dim['created_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            customer_dim['updated_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Customer dimension: {len(customer_dim)} records\")\n",
    "            return customer_dim\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _build_product_dimension(self, df):\n",
    "        \"\"\"Build product dimension\"\"\"\n",
    "        product_cols = ['product_id', 'product_name', 'category', 'sub_category']\n",
    "        available_cols = [col for col in product_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            product_dim = df[available_cols].drop_duplicates().reset_index(drop=True)\n",
    "            product_dim['product_id'] = range(1, len(product_dim) + 1)\n",
    "            product_dim['created_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            product_dim['updated_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Product dimension: {len(product_dim)} records\")\n",
    "            return product_dim\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _build_geography_dimension(self, df):\n",
    "        \"\"\"Build geography dimension\"\"\"\n",
    "        geo_cols = ['country', 'state', 'city', 'postal_code', 'region']\n",
    "        available_cols = [col for col in geo_cols if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            geography_dim = df[available_cols].drop_duplicates().reset_index(drop=True)\n",
    "            geography_dim['geography_id'] = range(1, len(geography_dim) + 1)\n",
    "            geography_dim['created_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Geography dimension: {len(geography_dim)} records\")\n",
    "            return geography_dim\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def _build_fact_orders(self, df, customer_dim, product_dim, geography_dim):\n",
    "        \"\"\"Build fact orders table\"\"\"\n",
    "        fact_orders = df.copy()\n",
    "        \n",
    "        # Add surrogate keys by joining with dimensions\n",
    "        if not customer_dim.empty and 'customer_id' in fact_orders.columns:\n",
    "            fact_orders = fact_orders.merge(\n",
    "                customer_dim[['customer_id', 'customer_id']], \n",
    "                on='customer_id', how='left'\n",
    "            )\n",
    "        \n",
    "        if not product_dim.empty and 'product_id' in fact_orders.columns:\n",
    "            fact_orders = fact_orders.merge(\n",
    "                product_dim[['product_id', 'product_id']], \n",
    "                on='product_id', how='left'\n",
    "            )\n",
    "        \n",
    "        if not geography_dim.empty:\n",
    "            geo_join_cols = ['country', 'state', 'city', 'postal_code']\n",
    "            available_geo_cols = [col for col in geo_join_cols if col in fact_orders.columns]\n",
    "            \n",
    "            if available_geo_cols:\n",
    "                fact_orders = fact_orders.merge(\n",
    "                    geography_dim[['geography_id'] + available_geo_cols],\n",
    "                    on=available_geo_cols, how='left'\n",
    "                )\n",
    "        \n",
    "        # Select final fact table columns\n",
    "        fact_columns = [\n",
    "            'order_id', 'customer_id', 'product_id', 'geography_id',\n",
    "            'order_date', 'ship_date', 'ship_mode', 'sales', 'quantity',\n",
    "            'discount', 'profit', 'delivery_days', 'row_id'\n",
    "        ]\n",
    "        \n",
    "        available_fact_cols = [col for col in fact_columns if col in fact_orders.columns]\n",
    "        fact_orders = fact_orders[available_fact_cols]\n",
    "        fact_orders['order_id'] = range(1, len(fact_orders) + 1)\n",
    "        fact_orders['load_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Fact orders: {len(fact_orders)} records\")\n",
    "        return fact_orders\n",
    "    \n",
    "    def _load_dimensions_to_db(self, customer_dim, product_dim, geography_dim):\n",
    "        \"\"\"Load dimension tables to database\"\"\"\n",
    "        with self.db_manager.get_connection() as conn:\n",
    "            if not customer_dim.empty:\n",
    "                customer_dim.to_sql('gold_dim_customer', conn, if_exists='replace', index=False)\n",
    "            \n",
    "            if not product_dim.empty:\n",
    "                product_dim.to_sql('gold_dim_product', conn, if_exists='replace', index=False)\n",
    "            \n",
    "            if not geography_dim.empty:\n",
    "                geography_dim.to_sql('gold_dim_geography', conn, if_exists='replace', index=False)\n",
    "        \n",
    "        self.logger.info(\"‚úÖ Dimensions loaded to database\")\n",
    "    \n",
    "    def _load_facts_to_db(self, fact_orders):\n",
    "        \"\"\"Load fact table to database\"\"\"\n",
    "        with self.db_manager.get_connection() as conn:\n",
    "            fact_orders.to_sql('gold_fact_orders', conn, if_exists='replace', index=False)\n",
    "        \n",
    "        self.logger.info(\"‚úÖ Facts loaded to database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da895f-af7f-486d-b4fc-0831c8a637ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE ORCHESTRATOR\n",
    "# ============================================================================\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = setup_logging()\n",
    "        self.db_manager = DatabaseManager(config.database_path)\n",
    "        \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Execute complete data pipeline\"\"\"\n",
    "        self.logger.info(\"üöÄ Starting Data Pipeline Execution\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Setup database\n",
    "            self._setup_database()\n",
    "            \n",
    "            # Step 2: Extract and process files (Bronze Layer)\n",
    "            bronze_data = self._bronze_layer_processing()\n",
    "            \n",
    "            # Step 3: Clean and validate data (Silver Layer)\n",
    "            silver_data = self._silver_layer_processing(bronze_data)\n",
    "            \n",
    "            # Step 4: Build dimensional model (Gold Layer)\n",
    "            gold_data = self._gold_layer_processing(silver_data)\n",
    "            \n",
    "            # Step 5: Generate reports\n",
    "            self._generate_pipeline_reports(bronze_data, silver_data, gold_data)\n",
    "            \n",
    "            self.logger.info(\"‚úÖ Pipeline execution completed successfully!\")\n",
    "            self.logger.info(\"=\"*60)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'bronze_records': len(bronze_data) if bronze_data is not None else 0,\n",
    "                'silver_records': len(silver_data) if silver_data is not None else 0,\n",
    "                'gold_records': gold_data\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Pipeline failed: {e}\")\n",
    "            return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    def _setup_database(self):\n",
    "        \"\"\"Setup database schemas\"\"\"\n",
    "        self.logger.info(\"üìä Setting up database...\")\n",
    "        self.db_manager.create_schemas()\n",
    "    \n",
    "    def _bronze_layer_processing(self):\n",
    "        \"\"\"Bronze layer - raw data ingestion\"\"\"\n",
    "        self.logger.info(\"ü•â Bronze Layer Processing...\")\n",
    "        \n",
    "        file_processor = FileProcessor(self.config.zip_file_path, self.config)\n",
    "        extracted_data, latest_files = file_processor.extract_and_get_latest_files()\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        if extracted_data:\n",
    "            combined_df = pd.concat(extracted_data, ignore_index=True)\n",
    "            \n",
    "            # Load to bronze layer\n",
    "            with self.db_manager.get_connection() as conn:\n",
    "                combined_df.to_sql('bronze_orders_raw', conn, if_exists='replace', index=False)\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Bronze layer: {len(combined_df)} records loaded\")\n",
    "            return combined_df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _silver_layer_processing(self, bronze_data):\n",
    "        \"\"\"Silver layer - data cleaning and validation\"\"\"\n",
    "        self.logger.info(\"ü•à Silver Layer Processing...\")\n",
    "        \n",
    "        if bronze_data is not None:\n",
    "            data_cleaner = DataCleaner()\n",
    "            silver_data = data_cleaner.clean_data(bronze_data)\n",
    "            \n",
    "            # Load to silver layer\n",
    "            with self.db_manager.get_connection() as conn:\n",
    "                silver_data.to_sql('silver_orders_clean', conn, if_exists='replace', index=False)\n",
    "            \n",
    "            self.logger.info(f\"‚úÖ Silver layer: {len(silver_data)} records processed\")\n",
    "            return silver_data\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _gold_layer_processing(self, silver_data):\n",
    "        \"\"\"Gold layer - dimensional modeling\"\"\"\n",
    "        self.logger.info(\"ü•á Gold Layer Processing...\")\n",
    "        \n",
    "        if silver_data is not None:\n",
    "            dim_builder = DimensionalModelBuilder(self.db_manager)\n",
    "            gold_data = dim_builder.build_dimensions_and_facts(silver_data)\n",
    "            \n",
    "            counts = {\n",
    "                'customers': len(gold_data['dim_customer']),\n",
    "                'products': len(gold_data['dim_product']),\n",
    "                'geography': len(gold_data['dim_geography']),\n",
    "                'orders': len(gold_data['fact_orders'])\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\"‚úÖ Gold layer: Dimensional model created\")\n",
    "            return counts\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _generate_pipeline_reports(self, bronze_data, silver_data, gold_data):\n",
    "        \"\"\"Generate pipeline execution reports\"\"\"\n",
    "        self.logger.info(\"üìã Generating pipeline reports...\")\n",
    "        \n",
    "        # Pipeline summary report\n",
    "        report = {\n",
    "            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'bronze_records': len(bronze_data) if bronze_data is not None else 0,\n",
    "            'silver_records': len(silver_data) if silver_data is not None else 0,\n",
    "            'gold_dim_customer': gold_data.get('customers', 0),\n",
    "            'gold_dim_product': gold_data.get('products', 0),\n",
    "            'gold_dim_geography': gold_data.get('geography', 0),\n",
    "            'gold_fact_orders': gold_data.get('orders', 0)\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        report_df = pd.DataFrame([report])\n",
    "        report_df.to_csv('pipeline_execution_report.csv', index=False)\n",
    "        \n",
    "        self.logger.info(\"‚úÖ Pipeline reports generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb703d-1234-42c4-853b-91fa999206e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS FOR MANUAL TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def test_pipeline_components():\n",
    "    \"\"\"Test individual pipeline components\"\"\"\n",
    "    print(\"üß™ Testing Pipeline Components...\")\n",
    "    \n",
    "    config = PipelineConfig()\n",
    "    \n",
    "    # Test file processor\n",
    "    if os.path.exists(config.zip_file_path):\n",
    "        file_processor = FileProcessor(config.zip_file_path, config)\n",
    "        try:\n",
    "            data, files = file_processor.extract_and_get_latest_files()\n",
    "            print(f\"‚úÖ File Processor: {len(data)} datasets extracted\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå File Processor failed: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
